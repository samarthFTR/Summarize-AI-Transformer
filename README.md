<p align="center">
  <h1 align="center">GenSumm-AI Transformer</h1>
  <p align="center">
    <strong>Summarize &amp; Generate Text with Dual Transformer Models</strong>
  </p>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI">
  <img src="https://img.shields.io/badge/Next.js_16-000000?style=for-the-badge&logo=next.js&logoColor=white" alt="Next.js">
  <img src="https://img.shields.io/badge/React_19-61DAFB?style=for-the-badge&logo=react&logoColor=black" alt="React">
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker">
  <img src="https://img.shields.io/badge/ğŸ¤—_HuggingFace-FFD21E?style=for-the-badge" alt="HuggingFace">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License">
</p>

---

**GenSumm-AI Transformer** is a production-ready, full-stack AI application that pairs two fine-tuned Transformer models â€” **T5-small** for abstractive text summarization and **GPT-2** for text generation. It delivers an end-to-end pipeline from data preprocessing and model training to inference, containerization, and cloud deployment.

> **Live Demo â†’** Frontend on [Vercel](https://vercel.com) Â· APIs on [Render](https://render.com) Â· Models on [HuggingFace Hub](https://huggingface.co/samarthftr)

---

## âœ¨ Key Features

| Category | Details |
|:---|:---|
| **Dual Transformer Architecture** | T5-small for abstractive summarization, GPT-2 for summary-to-text generation |
| **FastAPI Backends** | Two independent, high-performance APIs (ports `8000` & `8001`) with Swagger docs |
| **Modern Frontend** | Next.js 16 + React 19 monochrome UI with WebGL backgrounds (DarkVeil, Dither) and micro-animations |
| **Docker Ready** | Multi-stage Dockerfiles + Docker Compose for one-command stack deployment |
| **Cloud Native** | Render Blueprint for APIs, Vercel for frontend, HuggingFace Hub for model weights |
| **Modular Training** | Separated pipelines with configurable dataclass-based configs (`src/config.py`) |
| **GPU Acceleration** | CUDA-optimized training with FP16 mixed precision via ğŸ¤— Accelerate |
| **ROUGE Evaluation** | Integrated evaluation metrics for summarization quality assessment |
| **Interactive Demo** | Streamlit dashboard for quick local experimentation |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER / BROWSER                          â”‚
â”‚                    http://localhost:3000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Next.js 16    â”‚
                    â”‚   React 19 UI   â”‚
                    â”‚  (Vercel host)  â”‚
                    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                       â”‚          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Summarization â”‚  â”‚ Text Generator   â”‚
          â”‚ API (FastAPI) â”‚  â”‚ API (FastAPI)    â”‚
          â”‚  :8000        â”‚  â”‚  :8001           â”‚
          â”‚  T5-small     â”‚  â”‚  GPT-2           â”‚
          â”‚  (Render)     â”‚  â”‚  (Render)        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚       ğŸ¤— HuggingFace Hub            â”‚
          â”‚  samarthftr/summarizer  (242 MB)    â”‚
          â”‚  samarthftr/Text-generator (498 MB) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤— Pre-trained Models

Fine-tuned model weights are publicly hosted on HuggingFace Hub:

| Model | HuggingFace Repo | Parameters | Size | Task |
|:---|:---|:---|:---|:---|
| T5-small | [`samarthftr/summarizer`](https://huggingface.co/samarthftr/summarizer) | 60M | 242 MB | Abstractive Summarization |
| GPT-2 | [`samarthftr/Text-generator`](https://huggingface.co/samarthftr/Text-generator) | 124M | 498 MB | Text Generation |

Combined the models use **184M+** parameters.

---

## ğŸ› ï¸ Installation

### Prerequisites

| Tool | Version | Purpose |
|:---|:---|:---|
| Python | 3.11+ | Backend APIs & training |
| Node.js | 20+ | Frontend (Next.js) |
| Docker | Latest | Containerized deployment (optional) |
| CUDA | 11.8+ | GPU training (optional) |

### 1. Clone the Repository

```bash
git clone https://github.com/samarthFTR/Summarize-AI-Transformer.git
cd Summarize-AI-Transformer
```

### 2. Backend Setup

```bash
# Create and activate a virtual environment
python -m venv venv

# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate

# Install training dependencies (root-level)
pip install -r requirements.txt

# Install API-specific dependencies for each service
pip install -r summarization/requirements.txt
pip install -r text_generator/requirements.txt
```

### 3. Frontend Setup

```bash
cd frontend
npm install
```

### 4. Environment Variables

Copy the example env file and update with your API URLs:

```bash
cd frontend
cp .env.example .env.local
```

```env
# .env.local
NEXT_PUBLIC_API_URL=http://localhost:8000          # Summarization API
NEXT_PUBLIC_GENERATOR_API_URL=http://localhost:8001 # Text Generator API
```

---

## ğŸš€ Usage

### Training the Models

<details>
<summary><strong>Summarization (T5-small)</strong></summary>

```bash
cd summarization
python run.py
```

**Configuration** â€” edit `summarization/src/config.py`:
- `batch_size`: 16
- `num_epochs`: 3
- `learning_rate`: 5e-5
- `max_input_length`: 512
- `max_target_length`: 128
- `max_samples`: 5,000
- `fp16`: enabled

Training data: CSV with `Text` and `Summary` columns at `data/raw/data.csv`.

</details>

<details>
<summary><strong>Text Generation (GPT-2)</strong></summary>

```bash
cd text_generator
python run.py
```

**Configuration** â€” edit `text_generator/src/config.py`:
- `batch_size`: 4
- `num_epochs`: 3
- `learning_rate`: 5e-5
- `max_length`: 256
- `max_samples`: 2,000
- `fp16`: enabled

Training data: Shared XSum dataset from the summarization pipeline (reversed columns â€” Summary â†’ input, Text â†’ target).

**Generation Parameters:**
| Parameter | Default | Description |
|:---|:---|:---|
| `temperature` | 0.7 | Sampling temperature |
| `top_k` | 40 | Top-K filtering |
| `top_p` | 0.90 | Nucleus sampling |
| `repetition_penalty` | 1.4 | Penalize repeated tokens |
| `no_repeat_ngram_size` | 3 | Prevent 3-gram repetition |

</details>

### Starting the APIs

```bash
# Terminal 1 â€” Summarization API
cd summarization
python api.py
# â†’ http://localhost:8000
# â†’ Swagger Docs: http://localhost:8000/docs

# Terminal 2 â€” Text Generator API
cd text_generator
python api.py
# â†’ http://localhost:8001
# â†’ Swagger Docs: http://localhost:8001/docs
```

### Starting the Frontend

```bash
cd frontend
npm run dev
# â†’ http://localhost:3000
```

### Streamlit Demo (Summarization)

```bash
cd summarization
streamlit run app.py
```

---

## ğŸ“¡ API Reference

### Summarization API â€” `:8000`

| Method | Endpoint | Description |
|:---|:---|:---|
| `GET` | `/api/health` | Health check & model status |
| `POST` | `/api/summarize` | Generate a summary |

**POST `/api/summarize`** â€” Request Body:
```json
{
  "text": "Your long text to summarize...",
  "max_length": 128,
  "num_beams": 4
}
```

**Response:**
```json
{
  "summary": "Concise summary of the input text.",
  "input_length": 150,
  "output_length": 25,
  "processing_time": 1.234
}
```

### Text Generator API â€” `:8001`

| Method | Endpoint | Description |
|:---|:---|:---|
| `GET` | `/api/generate/health` | Health check & model status |
| `POST` | `/api/generate` | Generate expanded text |

**POST `/api/generate`** â€” Request Body:
```json
{
  "summary": "A brief summary or prompt to expand...",
  "max_length": 200,
  "temperature": 0.7,
  "top_k": 40,
  "top_p": 0.90
}
```

**Response:**
```json
{
  "generated_text": "Expanded full article from the summary...",
  "input_length": 12,
  "output_length": 85,
  "processing_time": 2.456
}
```

---

## ğŸ³ Docker Deployment

Run the entire stack with a single command:

```bash
# Build and start all 3 services
docker compose up --build

# Services:
#   Frontend           â†’ http://localhost:3000
#   Summarization API  â†’ http://localhost:8000
#   Text Generator API â†’ http://localhost:8001
```

Models are **pre-downloaded from HuggingFace Hub** during the Docker build stage, so container startup is fast. Each API image uses **CPU-only PyTorch** to keep image sizes minimal.

**Health Checks** are configured for both API services with 30s intervals and 60s start periods.

---

## â˜ï¸ Cloud Deployment

### APIs â†’ Render

1. Push code to GitHub
2. Go to [Render Dashboard](https://dashboard.render.com) â†’ **New** â†’ **Blueprint**
3. Connect your repo â€” Render auto-detects `render.yaml`
4. Click **Apply** to deploy both API services

The `render.yaml` blueprint configures:
- `summarize-ai-summarization` â€” T5 API on Docker runtime
- `summarize-ai-generator` â€” GPT-2 API on Docker runtime
- Oregon region, Starter plan, with health checks

### Frontend â†’ Vercel

1. Go to [Vercel](https://vercel.com) â†’ **Add New Project** â†’ Import repo
2. Set **Root Directory** to `frontend`
3. Add **Environment Variables**:
   ```
   NEXT_PUBLIC_API_URL=https://your-summarization-api.onrender.com
   NEXT_PUBLIC_GENERATOR_API_URL=https://your-generator-api.onrender.com
   ```
4. Click **Deploy**

### Models â†’ HuggingFace Hub

Upload fine-tuned models using the included utility:

```bash
# Login to HuggingFace
pip install huggingface_hub
huggingface-cli login

# Upload models
python upload_models.py
```

---

## ğŸ“‚ Project Structure

```
Summarize-AI-Transformer/
â”‚
â”œâ”€â”€ summarization/               # T5 Summarization Service
â”‚   â”œâ”€â”€ api.py                   # FastAPI server (port 8000)
â”‚   â”œâ”€â”€ app.py                   # Streamlit demo
â”‚   â”œâ”€â”€ run.py                   # Training entry point
â”‚   â”œâ”€â”€ Dockerfile               # CPU-optimized Docker image
â”‚   â”œâ”€â”€ requirements.txt         # API dependencies
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ config.py            # Dataclass-based configuration
â”‚       â”œâ”€â”€ data/                # Data loading & preprocessing
â”‚       â”œâ”€â”€ training/            # Training loop & utilities
â”‚       â”œâ”€â”€ inference/           # Inference logic
â”‚       â””â”€â”€ evaluation/          # ROUGE metric calculation
â”‚
â”œâ”€â”€ text_generator/              # GPT-2 Text Generation Service
â”‚   â”œâ”€â”€ api.py                   # FastAPI server (port 8001)
â”‚   â”œâ”€â”€ run.py                   # Training entry point
â”‚   â”œâ”€â”€ Dockerfile               # CPU-optimized Docker image
â”‚   â”œâ”€â”€ requirements.txt         # API dependencies
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ config.py            # Dataclass-based configuration
â”‚       â”œâ”€â”€ data/                # Data loading & tokenization
â”‚       â”œâ”€â”€ training/            # Training loop
â”‚       â””â”€â”€ inference/           # Text generation logic
â”‚
â”œâ”€â”€ frontend/                    # Next.js 16 + React 19 Frontend
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ page.js              # Landing page (Hero, Features, Architecture)
â”‚   â”‚   â”œâ”€â”€ summarize/page.js    # Summarization interface
â”‚   â”‚   â”œâ”€â”€ generate/page.js     # Text generation interface
â”‚   â”‚   â”œâ”€â”€ components/          # Navbar, Hero, Features, Footer, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ DarkVeil/        # WebGL background (landing page)
â”‚   â”‚   â”‚   â””â”€â”€ Dither/          # WebGL background (tool pages)
â”‚   â”‚   â””â”€â”€ globals.css          # Design system & monochrome theme
â”‚   â”œâ”€â”€ Dockerfile               # Multi-stage production build
â”‚   â”œâ”€â”€ .env.example             # Environment variable template
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ docker-compose.yml           # Orchestrate all 3 services
â”œâ”€â”€ render.yaml                  # Render Blueprint (auto-deploy)
â”œâ”€â”€ upload_models.py             # Push models to HuggingFace Hub
â”œâ”€â”€ requirements.txt             # Root training dependencies
â””â”€â”€ README.md
```

---

## ğŸ§° Tech Stack

| Layer | Technology |
|:---|:---|
| **Summarization Model** | T5-small (60M params) via ğŸ¤— Transformers |
| **Generation Model** | GPT-2 (124M params) via ğŸ¤— Transformers |
| **Training** | PyTorch 2.0+, ğŸ¤— Accelerate, FP16 mixed precision |
| **Evaluation** | ROUGE scores via ğŸ¤— Evaluate |
| **Backend** | FastAPI, Uvicorn, Pydantic v2 |
| **Frontend** | Next.js 16, React 19, CSS Modules |
| **Visual Effects** | Three.js, React Three Fiber, OGL (WebGL) |
| **Containerization** | Docker, Docker Compose |
| **Cloud (APIs)** | Render (Docker runtime) |
| **Cloud (Frontend)** | Vercel |
| **Model Hosting** | HuggingFace Hub |

---

## ğŸš§ Status & Roadmap

- [x] **Summarization Pipeline** â€” T5-small fine-tuned on XSum dataset
- [x] **Text Generation Pipeline** â€” GPT-2 fine-tuned with summary-to-text mapping
- [x] **Dual FastAPI Backends** â€” Independent, CORS-enabled, health-checked APIs
- [x] **Modern Frontend** â€” Monochrome UI with WebGL backgrounds & animations
- [x] **Docker Compose** â€” Full containerization with health checks
- [x] **HuggingFace Hub** â€” Model weights hosted & versioned
- [x] **Cloud Deployment** â€” Render + Vercel configurations ready
- [x] **Streamlit Demo** â€” Interactive summarization testing
- [ ] **Advanced Evaluation Metrics** â€” BLEU, METEOR, BERTScore
- [ ] **Batch Processing** â€” Multi-document summarization support
- [ ] **Model Comparison** â€” Side-by-side output from different model sizes

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the project
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [`LICENSE`](LICENSE) file for details.

---

<p align="center">
  Built with â¤ï¸ using Transformers, FastAPI, and Next.js
</p>
