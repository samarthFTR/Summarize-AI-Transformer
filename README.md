
# Summarize AI Transformer

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-wip-orange)

**Summarize AI Transformer** is a comprehensive toolkit for fine-tuning Transformer-based models (defaulting to T5-small) on text summarization tasks. It provides a modular pipeline for data loading, preprocessing, training, and evaluation, leveraging the power of Hugging Face `transformers` and `datasets`.

## ğŸš€ Key Features

*   **Transformer Architecture**: Built on top of the robust T5 (Text-to-Text Transfer Transformer) architecture.
*   **Modular Design**: Clean separation of concerns with dedicated modules for configuration, data handling, training, and inference.
*   **Configurable**: All hyperparameters (learning rate, batch size, model name) are easily adjustable via `src/config.py`.
*   **Evaluation Metrics**: Integrated ROUGE score calculation for reliable summarization quality assessment.
*   **Checkpointing**: Automatic model checkpointing during training to ensure progress is saved.

## ğŸ› ï¸ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/Summarize-AI-Transformer.git
    cd Summarize-AI-Transformer
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    # On Windows
    venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ“Š Data Preparation

The training pipeline expects a CSV file located at `data/raw/data.csv` with the following columns:

| Column Name | Description |
| :--- | :--- |
| `Text` | The full text article or document to be summarized. |
| `Summary` | The ground truth summary. |

*Note: You can configure the input file path and column names in `src/config.py`.*

## ğŸƒ Usage

### 1. Configuration
Adjust training parameters in `src/config.py`. Common settings include:
*   `max_input_length`: Maximum token length for input text.
*   `batch_size`: Batch size for training and evaluation.
*   `num_epochs`: Number of training epochs.
*   `model_name`: Pre-trained model identifier (e.g., `t5-small`, `t5-base`).

### 2. Training
To start the fine-tuning process, run the main script:

```bash
python run.py
```

This will:
1.  Load and preprocess the data.
2.  Fine-tune the model.
3.  Save checkpoints to `models/`.
4.  Log training metrics.

## ğŸ“‚ Project Structure

```plaintext
Summarize-AI-Transformer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Raw input data (data.csv)
â”‚   â””â”€â”€ processed/       # Processed datasets (optional)
â”œâ”€â”€ models/              # Saved model checkpoints
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py        # Central configuration file
â”‚   â”œâ”€â”€ data/            # Data loading and preprocessing scripts
â”‚   â”œâ”€â”€ training/        # Training loop and utilities
â”‚   â”‚   â”œâ”€â”€ train.py     # Main training logic
â”‚   â”‚   â””â”€â”€ trainer_utils.py # Helper functions for Trainer
â”‚   â”œâ”€â”€ inference/       # Inference scripts (WIP)
â”‚   â””â”€â”€ evaluation/      # Evaluation metrics (WIP)
â”œâ”€â”€ run.py               # Entry point for the application
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation
```

## ğŸš§ Status & Roadmap

*   âœ… **Training Pipeline**: Fully implemented support for T5 fine-tuning.
*   âœ… **Evaluation**: ROUGE score integration is active during training.
*   ğŸš§ **Inference**: Scripts for generating summaries from trained models are currently under development (`src/inference/`).
*   ğŸš§ **Evaluation Script**: Standalone evaluation script is planned (`src/evaluation/`).

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for details.
