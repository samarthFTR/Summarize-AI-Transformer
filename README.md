# GenSumm-AI Transformer

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.95%2B-009688)
![Next.js](https://img.shields.io/badge/Next.js-16-black)
![Docker](https://img.shields.io/badge/Docker-ready-2496ED)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Models-yellow)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-active-brightgreen)

**GenSumm-AI Transformer** is a comprehensive, full-stack AI toolkit featuring two Transformer-based models â€” **T5-small** for text summarization and **GPT-2** for text generation. It provides a complete pipeline from training to production deployment.

This project offers: data preprocessing, fine-tuning, inference via **FastAPI** backends, a modern **Next.js** frontend, and full **Docker** containerization with deployment support for **Render** and **Vercel**.

## ğŸš€ Key Features

*   **Dual Transformer Architecture**:
    *   **T5 (Text-to-Text Transfer Transformer)** â€” fine-tuned for abstractive text summarization
    *   **GPT-2** â€” fine-tuned for text generation from summaries
*   **Modular Training Pipelines**: Clean separation of concerns for data loading, training, evaluation, and inference.
*   **Full-Stack Application**:
    *   **Backend**: Two high-performance APIs built with **FastAPI** (port 8000 & 8001)
    *   **Frontend**: Sleek, monochrome UI built with **Next.js 16** and **React 19**
    *   **Demo**: Interactive **Streamlit** dashboard for quick testing
*   **Production-Ready Deployment**:
    *   ğŸ³ **Docker** + **Docker Compose** for containerized deployment
    *   â˜ï¸ **Render** for API hosting (with Blueprint auto-deploy)
    *   â–² **Vercel** for frontend hosting
    *   ğŸ¤— **HuggingFace Hub** for model weight storage
*   **Configurable**: Centralized configuration management via `src/config.py` in each service
*   **Evaluation Metrics**: Integrated **ROUGE** score calculation for quality assessment
*   **GPU Acceleration**: Optimized for CUDA-enabled environments for faster training

## ğŸ¤— Pre-trained Models

Fine-tuned model weights are hosted on HuggingFace Hub:

| Model | HuggingFace Repo | Size | Task |
| :--- | :--- | :--- | :--- |
| T5-small | [samarthftr/summarizer](https://huggingface.co/samarthftr/summarizer) | 242 MB | Text Summarization |
| GPT-2 | [samarthftr/Text-generator](https://huggingface.co/samarthftr/Text-generator) | 498 MB | Text Generation |

## ğŸ› ï¸ Installation

### Prerequisites
*   Python 3.8+
*   Node.js 18+ (for frontend)
*   Docker (optional, for containerized deployment)

### 1. Clone & Setup

```bash
git clone https://github.com/samarthFTR/Summarize-AI-Transformer.git
cd Summarize-AI-Transformer
```

### 2. Backend Setup (Python APIs)

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate
# Activate (macOS/Linux)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Frontend Setup (Next.js)

```bash
cd frontend
npm install
```

## ğŸƒ Usage

### 1. Training the Models

**Summarization (T5):**
```bash
cd summarization
python run.py
```

**Text Generator (GPT-2):**
```bash
cd text_generator
python run.py
```

### 2. Starting the Backend APIs

**Summarization API (port 8000):**
```bash
cd summarization
python api.py
```
*   ğŸ“„ API Docs: `http://localhost:8000/docs`
*   â¤ï¸ Health Check: `http://localhost:8000/api/health`

**Text Generator API (port 8001):**
```bash
cd text_generator
python api.py
```
*   ğŸ“„ API Docs: `http://localhost:8001/docs`
*   â¤ï¸ Health Check: `http://localhost:8001/api/generate/health`

### 3. Running the Frontend

```bash
cd frontend
npm run dev
```
*Access the application at `http://localhost:3000`.*

### 4. Running the Streamlit Demo

```bash
cd summarization
streamlit run app.py
```

## ğŸ³ Docker Deployment

Run the entire stack with Docker Compose:

```bash
# Build and start all services
docker compose up --build

# Services will be available at:
# Frontend:           http://localhost:3000
# Summarization API:  http://localhost:8000
# Text Generator API: http://localhost:8001
```

Models are automatically downloaded from HuggingFace Hub during the Docker build.

## â˜ï¸ Cloud Deployment

### APIs â†’ Render

1.  Push code to GitHub
2.  Go to [Render Dashboard](https://dashboard.render.com) â†’ **New** â†’ **Blueprint**
3.  Connect your GitHub repo â€” Render auto-detects `render.yaml`
4.  Click **Apply** to deploy both API services

### Frontend â†’ Vercel

1.  Go to [Vercel](https://vercel.com) â†’ **Add New Project** â†’ Import repo
2.  Set **Root Directory** to `frontend`
3.  Add **Environment Variables**:
    ```
    NEXT_PUBLIC_API_URL=https://your-summarization-api.onrender.com
    NEXT_PUBLIC_GENERATOR_API_URL=https://your-generator-api.onrender.com
    ```
4.  Click **Deploy**

## ğŸ“Š Data Preparation

The training pipelines require a CSV file with the following columns:

| Column Name | Description |
| :--- | :--- |
| `Text` | The full text article or document to be summarized. |
| `Summary` | The ground truth summary. |

*Configure input paths and column mapping in each service's `src/config.py`.*

## ğŸ“‚ Project Structure

```plaintext
Summarize-AI-Transformer/
â”œâ”€â”€ summarization/           # T5 Summarization Service
â”‚   â”œâ”€â”€ api.py               # FastAPI backend (port 8000)
â”‚   â”œâ”€â”€ app.py               # Streamlit demo
â”‚   â”œâ”€â”€ run.py               # Training entry point
â”‚   â”œâ”€â”€ Dockerfile           # Docker image definition
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config.py        # Service configuration
â”‚   â”‚   â”œâ”€â”€ data/            # Data loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ training/        # Training loop & utilities
â”‚   â”‚   â”œâ”€â”€ inference/       # Inference logic
â”‚   â”‚   â””â”€â”€ evaluation/      # ROUGE metric calculation
â”‚   â””â”€â”€ models/              # Saved model checkpoints (gitignored)
â”‚
â”œâ”€â”€ text_generator/          # GPT-2 Text Generation Service
â”‚   â”œâ”€â”€ api.py               # FastAPI backend (port 8001)
â”‚   â”œâ”€â”€ run.py               # Training entry point
â”‚   â”œâ”€â”€ Dockerfile           # Docker image definition
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config.py        # Service configuration
â”‚   â”‚   â”œâ”€â”€ data/            # Data loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ training/        # Training loop
â”‚   â”‚   â””â”€â”€ inference/       # Text generation logic
â”‚   â””â”€â”€ models/              # Saved model checkpoints (gitignored)
â”‚
â”œâ”€â”€ frontend/                # Next.js 16 Frontend
â”‚   â”œâ”€â”€ src/                 # React components & pages
â”‚   â”œâ”€â”€ public/              # Static assets
â”‚   â”œâ”€â”€ Dockerfile           # Multi-stage Docker build
â”‚   â””â”€â”€ package.json         # Node.js dependencies
â”‚
â”œâ”€â”€ docker-compose.yml       # Orchestrate all 3 services
â”œâ”€â”€ render.yaml              # Render Blueprint (auto-deploy)
â”œâ”€â”€ upload_models.py         # Upload models to HuggingFace Hub
â”œâ”€â”€ requirements.txt         # Root Python dependencies (training)
â””â”€â”€ README.md
```

## ğŸš§ Status & Roadmap

*   âœ… **Summarization Pipeline**: T5-small fine-tuning complete
*   âœ… **Text Generation Pipeline**: GPT-2 fine-tuning complete
*   âœ… **Backend APIs**: Dual FastAPI services
*   âœ… **Frontend**: Modern Next.js 16 UI with React 19
*   âœ… **Docker**: Full containerization with Docker Compose
*   âœ… **HuggingFace Hub**: Model weights hosted and versioned
*   âœ… **Cloud Deployment**: Render + Vercel configurations ready
*   ğŸš§ **Advanced Metrics**: Additional evaluation metrics planned

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for details.
